# minION-scripts
Welcome to the repository for the ont-derived microbial community analysis pipeline developed at the SSB+Metagen Labs (FMRP-USP / FFCLRP-USP)!

# Why?
As of the time of writing, there is no single unified approach to process and analyze 16S data derived from nanopore sequencing (a thorough review on the matter was recently written by [Santos and colleagues](https://www.sciencedirect.com/science/article/pii/S2001037019303745)). In this repository we describe the steps we take to carry our in-house analyses of nanopore sequencing data. Suggestions/comments are always welcome!

# How does our analysis work?
The upstream process to a microbial community analysis, i.e. taxonomic assignment, is comprised of four main parts:
1. Accessing the basecalled reads (generated by guppy)
2. Filtering these reads based on well-defined parameters (quality control)
3. Aligning the reads against a reference 16S database
4. Parsing the alignment results to retrieve taxonomy information  

Each of these steps will be expanded upon in the following sections.  

## 1. Accessing the basecalled reads

By default, guppy will store basecalled reads (along with raw .fast5 data, logs...) into a complex hierarchy of directories starting with the name of the experiment parent as the parent directory. If demultiplexing was enabled during basecalling, the reads will be sorted and stored in directories referring to the barcode detected in that read. If a quality score threshold has been set (the default is 7), guppy will divide reads further into "passed" or "failed" based on this threshold (a more complete description can be found in the well-documented [Guppy Manual](https://community.nanoporetech.com/protocols/Guppy-protocol/v/GPB_2003_v1_revU_14Dec2018) (requires login)).

A typical run output can look like this (shortened for brevity):

```
experiment_name/
├── sample_name
│   └── 20201103_1311_MN34450_AEM357_a8311dc0
│       ├── drift_correction_AEM357_f157e82d.csv
│       ├── duty_time_AEM357_f157e82d.csv
│       ├── fast5_fail
│       │   ├── barcode01
│       │   │   ├── AEM357_fail_barcode01_f157e82d_0.fast5
│       │   │   └── AEM357_fail_barcode01_f157e82d_1.fast5
│       │   ├── barcode02
│       │   │   ├── AEM357_fail_barcode02_f157e82d_0.fast5
│       │   │   └── AEM357_fail_barcode02_f157e82d_1.fast5
...
│       │   └── unclassified
│       │       ├── AEM357_fail_unclassified_f157e82d_0.fast5
│       │       ├── AEM357_fail_unclassified_f157e82d_10.fast5
...
│       ├── fast5_pass
│       │   ├── barcode01
│       │   │   ├── AEM357_pass_barcode01_f157e82d_0.fast5
│       │   │   ├── AEM357_pass_barcode01_f157e82d_1.fast5
│       │   ├── barcode02
│       │   │   ├── AEM357_pass_barcode02_f157e82d_0.fast5
│       │   │   ├── AEM357_pass_barcode02_f157e82d_10.fast5
...
│       │   └── unclassified
│       │       ├── AEM357_pass_unclassified_f157e82d_0.fast5
│       │       ├── AEM357_pass_unclassified_f157e82d_10.fast5
...
│       ├── fast5_skip
│       │   └── AEM357_skip__f157e82d_0.fast5
│       ├── fastq_fail
│       │   ├── barcode01
│       │   │   ├── AEM357_fail_barcode01_f157e82d_0.fastq
│       │   │   └── AEM357_fail_barcode01_f157e82d_1.fastq
│       │   ├── barcode02
│       │   │   ├── AEM357_fail_barcode02_f157e82d_0.fastq
│       │   │   └── AEM357_fail_barcode02_f157e82d_1.fastq
...
│       ├── fastq_pass
│       │   ├── barcode01
│       │   │   ├── AEM357_pass_barcode01_f157e82d_0.fastq
│       │   │   ├── AEM357_pass_barcode01_f157e82d_1.fastq
│       │   │   ├── AEM357_pass_barcode01_f157e82d_2.fastq
│       │   ├── barcode02
│       │   │   ├── AEM357_pass_barcode02_f157e82d_0.fastq
│       │   │   ├── AEM357_pass_barcode02_f157e82d_10.fastq
│       │   │   ├── AEM357_pass_barcode02_f157e82d_1.fastq
...
│       ├── final_summary_AEM357_f157e82d.txt
│       ├── mux_scan_data_AEM357_f157e82d.csv
│       ├── report_AEM357_20201103_1312_a8311dc0.md
│       ├── sequencing_summary_AEM357_f157e82d.txt
│       └── throughput_AEM357_f157e82d.csv
└── report__20201103_1011_a8311dc0.pdf
```

It is easy to see that with multiple barcodes and high throughput an experiment folder may grow considerably.

For our purposes, we only want to retrieve the contents of the `fastq_pass/` subdirectory. For that, we have developed a pair of scripts: `merge-fastq.sh` and `retrieve-directories.sh` that can be called from anywhere (e.g. to a freshly created analysis directory in `/home/Documents`). `merge-fastq.sh` provides a function that is able to programatically copy the fastq files to tidy directories in the working directory it was called. These tidy directories retain information on the run (flowcell id and experiment date) and contain a single fastq file (a concatenated version of the pre-existing files). `retrieve-directories.sh` takes as an argument a directory and generates a list of "_passed" subdirectories containing .fastq files within that directory's tree. Finally, it calls the merge-fastq function on each of these directories. This effectively retrieves .fastq files of interest to the current working directory.

To simplify the automatization of analysis, there is a wrapper called `get-fastq.sh` that runs both scripts. Run this command from the same directory where the `scripts` directory from this repository is located:

```
source ./scripts/get-fastq.sh $1 $2 $3
```
- `$1`: is the name of the directory analysis will be carried (for instance microbiome_analysis. This directory will be created if not already)  
- `$2`: is a prefix with which to search for directories in the `minknow/data/` directory (the default save path for sequencing runs within minknow). This can be useful to batch-run the script for a series of experiments (if you named them properly)  
- `$3`: is an optional argument providing a location where to search for the output data (defaults to the minknow default output).
  
__NOTE: these arguments currently follow this strict order. In the future we intend to implement (more sophisticated) named arguments__ 


## 2. Quality control of the reads
_(TODO)_
